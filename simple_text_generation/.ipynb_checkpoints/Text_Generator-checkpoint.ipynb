{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple text generator using LSTM layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadText():\n",
    "    print('reading data')\n",
    "    # with open('../fox_scrapes/deadpool2_online_articles.txt', \"r\") as text_file:\n",
    "    with open('/Users/nikolaushbuenning/ebooks/great_expectations', \"r\") as text_file:\n",
    "        data = text_file.read().lower()\n",
    "    data = data.replace('--nextarticle', '')\n",
    "    text = list(data)\n",
    "    inputSize = len(text)\n",
    "    data = sorted(list(set(text)))\n",
    "    dataSize = len(data)\n",
    "    return text, inputSize, dataSize, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and one hot encode the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "done reading data\n",
      "looping through text, creating sentence\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "text, textSize, uniqueChars, chars = LoadText()\n",
    "print('done reading data')\n",
    "ci = dict((c,i) for i, c in enumerate(chars))\n",
    "ic = dict((i,c) for i, c in enumerate(chars))\n",
    "\n",
    "seqLen = 80\n",
    "sentences = []\n",
    "nextChars = []\n",
    "print('looping through text, creating sentence')\n",
    "for i in range(0, textSize-seqLen, 3):\n",
    "    sentences.append(text[i:i+seqLen])\n",
    "    nextChars.append(text[i+seqLen])\n",
    "\n",
    "x_train = np.zeros((len(sentences), seqLen, uniqueChars), dtype=np.bool)\n",
    "y_train = np.zeros((len(sentences), uniqueChars), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, char in enumerate(sentence):\n",
    "        x_train[i, j, ci[char]] = 1\n",
    "    y_train[i, ci[nextChars[i]]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n"
     ]
    }
   ],
   "source": [
    "print('building model')\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape=(seqLen, uniqueChars), return_sequences=True, implementation=1))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(LSTM(150, input_shape=(seqLen, uniqueChars), implementation=1))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(uniqueChars, activation='softmax'))\n",
    "model.compile(optimizer=keras.optimizers.rmsprop(lr=0.007), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train and output after each 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "n_iters = 5\n",
    "\n",
    "print('iterating')\n",
    "file_obj = open('generated.txt', 'w')\n",
    "for it in range(n_iters):\n",
    "    print('=' * 50)\n",
    "    print('iteration: ', it)\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=n_epochs, verbose=1)\n",
    "    startIndex = np.random.randint(0, textSize-seqLen-1)\n",
    "    generated = ''\n",
    "    sentence = text[startIndex:startIndex+seqLen]\n",
    "    generated = generated.join(sentence)\n",
    "    print(\"generating with seed: \", generated)\n",
    "    if it > -1:\n",
    "        for i in range(4000):\n",
    "            x = np.zeros((1, seqLen, uniqueChars))\n",
    "            for j, char in enumerate(sentence):\n",
    "                x[0, j, ci[char]] = 1\n",
    "\n",
    "            prediction = model.predict(x, batch_size=1, verbose=0)[0]\n",
    "            index = np.random.choice(uniqueChars, p=prediction)\n",
    "            # print([index, prediction])\n",
    "            char = ic[index]\n",
    "            generated += char\n",
    "            sentence.append(char)\n",
    "            sentence.pop(0)\n",
    "        file_obj.write('--------- next generated text')\n",
    "        file_obj.write(generated)\n",
    "    print('generated text: ')\n",
    "    print(generated)\n",
    "file_obj.close()\n",
    "# model.save('predator_model0.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or one training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "331526/331526 [==============================] - 1344s 4ms/step - loss: 1.8217\n",
      "Epoch 2/20\n",
      "331526/331526 [==============================] - 1883s 6ms/step - loss: 1.5836\n",
      "Epoch 3/20\n",
      "331526/331526 [==============================] - 1439s 4ms/step - loss: 1.5161\n",
      "Epoch 4/20\n",
      "331526/331526 [==============================] - 1385s 4ms/step - loss: 1.4823\n",
      "Epoch 5/20\n",
      "331526/331526 [==============================] - 1368s 4ms/step - loss: 1.4568\n",
      "Epoch 6/20\n",
      "331526/331526 [==============================] - 1239s 4ms/step - loss: 1.4405\n",
      "Epoch 7/20\n",
      "331526/331526 [==============================] - 1282s 4ms/step - loss: 1.4260\n",
      "Epoch 8/20\n",
      "331526/331526 [==============================] - 1346s 4ms/step - loss: 1.4134\n",
      "Epoch 9/20\n",
      "331526/331526 [==============================] - 1368s 4ms/step - loss: 1.4047\n",
      "Epoch 10/20\n",
      "331526/331526 [==============================] - 1368s 4ms/step - loss: 1.3993\n",
      "Epoch 11/20\n",
      "331526/331526 [==============================] - 1287s 4ms/step - loss: 1.3978\n",
      "Epoch 12/20\n",
      "331526/331526 [==============================] - 1265s 4ms/step - loss: 1.3938\n",
      "Epoch 13/20\n",
      "331526/331526 [==============================] - 1278s 4ms/step - loss: 1.4075\n",
      "Epoch 14/20\n",
      "331526/331526 [==============================] - 29159s 88ms/step - loss: 1.4037\n",
      "Epoch 15/20\n",
      "331526/331526 [==============================] - 1232s 4ms/step - loss: 1.3997\n",
      "Epoch 16/20\n",
      "331526/331526 [==============================] - 1313s 4ms/step - loss: 1.3858\n",
      "Epoch 17/20\n",
      "331526/331526 [==============================] - 1308s 4ms/step - loss: 1.3856\n",
      "Epoch 18/20\n",
      "331526/331526 [==============================] - 1352s 4ms/step - loss: 1.3897\n",
      "Epoch 19/20\n",
      "331526/331526 [==============================] - 1268s 4ms/step - loss: 1.4073\n",
      "Epoch 20/20\n",
      "331526/331526 [==============================] - 1326s 4ms/step - loss: 1.3983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199c45ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndex = np.random.randint(0, textSize-seqLen-1)\n",
    "generated = ''\n",
    "sentence = text[startIndex:startIndex+seqLen]\n",
    "generated = generated.join(sentence)\n",
    "for i in range(4000):\n",
    "    x = np.zeros((1, seqLen, uniqueChars))\n",
    "    for j, char in enumerate(sentence):\n",
    "        x[0, j, ci[char]] = 1\n",
    "\n",
    "    prediction = model.predict(x, batch_size=1, verbose=0)[0]\n",
    "    index = np.random.choice(uniqueChars, p=prediction)\n",
    "    # print([index, prediction])\n",
    "    char = ic[index]\n",
    "    generated += char\n",
    "    sentence.append(char)\n",
    "    sentence.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l, and had been for\n",
      "some time silently meeting mr. jaggers's look. when i did at the knobling even a fattentions i lose the intentions at that of mlountital night, what i understand it\n",
      "a sided worstance to cutleth that he is, i am\n",
      "very oster either as!”\n",
      "\n",
      "i had not fele. which there were hands as to suppose having been it, but for her eges. he had no would try into my sister's. “of long hurt?” he and we being at the christy to the water return at have in the day, only complated down from the house joe and not concarsed to make us, i replied with a coysfaded on a gall dumprocked restlemen. i one occursed surpiigh to all his\n",
      "now, which full as exhent medily afoched of the same knowledges, the palect of me us aa no, you have been knew?” sit inso bridenbod towards no little garding and but\n",
      "i opened me sat an agke\n",
      "of fire to-fortute very\n",
      "garden persons and rich-morry about the some moment, or\n",
      "became every hit, in somewed him. provinfness. would pretty brought it\n",
      "the eluvin boby, master, and arm that eyoung tide herbert bepardoar\n",
      "to sigh, in the head in the wopsle, insteloved as that it miting,” said you, one\n",
      "outjoe, “and the cold of alness had a latter miss havisham, and you ofcice me and live as chocked out of them to be to say which had been trabbing in a piring.\n",
      "\n",
      "me. but he would be weared up, that i was miner and unlone at the\n",
      "breast for me, and so was much that i had no present yaid of somewhich i might somethink, and to come concinations,” said miss store my meant jurxe in that revated, so,\n",
      "how will make you leaned by memory the green and again, and  “first! you give the seemed for\n",
      "taking and marked. what the cals were\n",
      "a sstcaid. \n",
      "knew\n",
      "it was a turn to me, in\n",
      "set\n",
      "under his loom and have knew\n",
      "most by “and\n",
      "mlutty wribbist and left hand to undertak, and i.”\n",
      "\n",
      "“how, here now.”\n",
      "\n",
      "“oh,” returned the dark into our faint after.\n",
      "\n",
      "“do it that of thet hove an hour and raybemen--trouble shilling all and but mr into a gentleman! that i had a black full of taker of jown was to me her should have had recogniled it,” said i, “to his\n",
      "effencl of me, so miss havisham had\n",
      "pleasantly reference; “no conaunce but not in me in a josemings since my being mrcothing of the odd pounds, cro gentleman, and pleasare. but the sight cweir how to him and taken to seganing, sham added with any people knew with it's furnster, as meditating\n",
      "which we was one me. i seall you.--“is a very our single tale at the sverovior\n",
      "'ound with it, in the polent life in his manner, doly rrobs of the such shilling to the chimney-with me, in solvee\n",
      "still\n",
      "engords. lay, looking at me, and when i had onberbed to the regard aurd readfully once wemmick. by graving a more fiers, in that perisech open afrlacped\n",
      "became with provis, and it\n",
      "would have been a jored the exprisity for me that my sister seppired my shoulder of evening. when miss havisham of everys you.”\n",
      "\n",
      "“yes. from seem, bighoind by, making my\n",
      "tording at yourself upon the hat, and that i saw them that i broken mrt my elbow what,” said mrcelight writting, “i\n",
      "understand her with the surpose back, and said gown to a last sig't be too sloughed away in the length again. mi havy been intended, the morning\n",
      "on it, and he did purpoted to appilonel condainer,\n",
      "both\n",
      "in the\n",
      "satularger mover that stairs in his rarm and biddy and names.\n",
      "\n",
      "“when it was standed a\n",
      "give small,\n",
      "here's try mought and\n",
      "one of one on soir. you miss havisham, she had not mr. jaggers, old from dead, eater astenterk in his pantress away, melg\n",
      "somethink to know, indeed. if you knowt it of it manable my day of nevisband, after having taken inormon with his work known all the same whisk into me?” drand pip,” i alonger, then stip'mis it open, when she was to come from it, and we hopeful morse, and his head us, perties, not some mine. and do up miss havisham affictated from the prospooitenness, in look on and then but i both the short\n",
      "like a pemine, sripping it and appearanted his eyes\n",
      "out of ten pains, and again on, stalk in a gay people name.\n",
      "“ohvy calco o ruating round part of me a\n",
      "semdil position. and that tic biddy in a dead on itself with it, i should\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok, so not perfect. But, it kind of works; may need some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
